<html dir="ltr" data-code-prettify="" data-mobile-support="" data-search=""><head>
<title>在Spring中使用Apache Kafka进行集成和数据处理管道</title>
<meta id="Viewport" name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1, minimum-scale=1, user-scalable=no">
<link rel="shortcut icon" type="image/x-icon" href="/img/favicon-ca31b78daf0dd9a106bbf3c6d87d4ec7.png">
<link href="asset?aid=0" rel="stylesheet" type="text/css">
<link rel="stylesheet" type="text/css" href="/css/main-bc256dba5f9d253d6425441ccfb82576.css">
<script src="/jspm_packages/system-eccc019329febb5a1b06bde008ca5614.js"></script>
<script>System.config({baseURL: "/b92013b"});</script>
<script src="/config-5a675c9cddea3a5f55b71416e67d47d6.js"></script>
<script>System.import('app/main.js')</script>
<meta name="google-site-verification" content="7qGntFPD9lWAVCtUu5U77v4l68PsTHf6xpzgjQv2j2M">

<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
      j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
      'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-KZM7GF6');</script>

<link href="/css/blog-92993c3ec6808bded45b277c18d7d621.css" rel="stylesheet" type="text/css">

<meta name="twitter:card" content="summary">
<meta name="twitter:site" content="@springcentral">
<meta name="twitter:title" content=" Using Apache Kafka for Integration and Data Processing Pipelines with Spring">
<meta name="twitter:description" content="<p>Applications generated more and more data than ever before and a huge part of the challenge - before it can even be analyzed - is accommodating the load in the first place. <a href=" http:="" ="" kafka.apache.or="="></head><body dir="ltr">Apache’s Kafka meets this challenge. It was originally designed by LinkedIn and subsequently open-sourced in 2011. The project aims to provide a unified, high-throughput, low-latency platform for handling real-time data feeds. The design is heavily influenced by transaction logs. It is a messaging system, similar to traditional messaging systems like RabbitMQ, ActiveMQ, MQSeries, but it’s ideal for log aggregation, persistent messaging, fast (_hundreds_ of megabytes per second!) reads and writes, and can accommodate numerous clients. Naturally, this makes it <em>perfect</em> for cloud-scale architectures!
">
<meta name="twitter:creator" content="@starbuxman">
<meta name="twitter:image:src" content="https://gravatar.com/avatar/fb22593caf24e4bb4c98d467cdd247e6?s=200">

<meta property="og:title" content=" Using Apache Kafka for Integration and Data Processing Pipelines with Spring">
<meta property="og:image" content="https://gravatar.com/avatar/fb22593caf24e4bb4c98d467cdd247e6?s=200">
<meta property="og:description" content="<p>Applications generated more and more data than ever before and a huge part of the challenge - before it can even be analyzed - is accommodating the load in the first place. <a href=" http:="" ="" kafka.apache.or="=">Apache’s Kafka meets this challenge. It was originally designed by LinkedIn and subsequently open-sourced in 2011. The project aims to provide a unified, high-throughput, low-latency platform for handling real-time data feeds. The design is heavily influenced by transaction logs. It is a messaging system, similar to traditional messaging systems like RabbitMQ, ActiveMQ, MQSeries, but it’s ideal for log aggregation, persistent messaging, fast (_hundreds_ of megabytes per second!) reads and writes, and can accommodate numerous clients. Naturally, this makes it <em>perfect</em> for cloud-scale architectures!
">
<meta content="article" property="og:type">
<meta property="og:article:published_time" content="2015-04-15 00:46:00.0">



<noscript><iframe height="0" src="https://www.googletagmanager.com/ns.html?id=GTM-KZM7GF6" width="0" style="display:none"></iframe></noscript>

<script type="text/javascript">// Work around Google font rendering issues in webkit browsers on Windows 7
    if (navigator.userAgent.indexOf("NT 6.1") != -1) {
      document.body.style.WebkitTextStroke = "0.5px";
    }</script>
<div class="viewport">
<header class="navbar header--navbar desktop-only">
<div class="navbar-inner">
<div class="container-fluid">
<div class="spring-logo--container">
<a class="spring-logo" href="/"><span></span></a>
</div>
<ul class="nav pull-right">
<li class="navbar-link">
<a href="/projects">专案</a>
</li>
<li class="navbar-link">
<a href="/guides">导游</a>
</li>
<li class="navbar-link active">
<a href="/blog">博客</a>
</li>
<li class="navbar-link">
<a href="/services">培训与认证</a>
</li>
<li class="navbar-link nav-search js-nav-search">
<a>
<i class="icon-search navbar-search--icon js-search-input-open"></i>
<span class="search-input-close js-search-input-close">
<i class="icon-remove"></i>
</span>
</a>
</li>
</ul>
</div>
</div>
<div class="search-dropdown--container js-search-dropdown">
<div class="container-fluid">
<div class="search-form--container">
<form class="form-inline form-search" action="/search" method="get">
<input class="search-query search-form--form js-search-input" name="q" placeholder="搜索文档，指南和帖子..." type="text" value="">
<button class="search-form--submit" type="submit"></button>
</form>
</div>
</div>
</div>
</header>
<div>
<div class="mobile-navigation--wrapper mobile-only">
<div class="navigation-drawer--container">
<div class="mobile-search--container">
<form class="form-inline form-search" action="/search" method="get">
<button class="search-form--submit" type="submit"></button>
<input class="search-query search-form--form js-search-input" name="q" placeholder="搜索..." type="text" value="">
</form>
</div>
<div class="navigation-item-list">
<div class="navbar-link">
<a href="/">家<i class="icon-chevron-right pull-right"></i>
</a>
</div>
<div class="navbar-link">
<a href="/projects">专案<i class="icon-chevron-right pull-right"></i>
</a>
</div>
<div class="navbar-link">
<a href="/guides">导游<i class="icon-chevron-right pull-right"></i>
</a>
</div>
<div class="navbar-link active">
<a href="/blog">博客<i class="icon-chevron-right pull-right"></i>
</a>
</div>
<div class="navbar-link">
<a href="/services">培训与认证<i class="icon-chevron-right pull-right"></i>
</a>
</div>
</div>
</div>
<div class="mobile-nav">
<div class="nav-icon js-open-nav-drawer">
<i class="icon-reorder"></i>
</div>
<div class="header-center-icon">
<a href="/">
<div class="icon icon-spring-logo-mobile"></div>
</a>
</div>
</div>
</div>
</div>
<div class="header--container"></div>
<div class="container-fluid"></div>
<div></div>
<div class="container-fluid">
<div class="main-body--wrapper">
<div class="row-fluid blog--wrapper">
<article class="span8 mobile-left-pane" id="content">
<header class="desktop-only">
<div class="blog-category active content--title">
<div>工程</div>
</div>
<div class="blog-category content--title">
<div>发布</div>
</div>
<div class="blog-category content--title">
<div>新闻与活动</div>
</div>
</header>
<div class="blog--container">
<header>
<h1 class="blog--title">在Spring中使用Apache Kafka进行集成和数据处理管道</h1>
<div class="meta-data--container">
<div class="meta-data--item desktop-only">
<div class="meta-data--icon icon blog-icon engineering"></div>
<a class="category">工程</a>
</div>
<div class="meta-data--item">
<img class="meta-data--icon" src="https://gravatar.com/avatar/fb22593caf24e4bb4c98d467cdd247e6?s=20&d=mm"> <a class="author" rel="author" href="/team/jlong">乔什·朗（Josh Long）</a>
</div>
<div class="meta-data--item">
<div class="meta-data--icon icon blog-icon calendar"></div>
<time class="date" pubdate="" datetime="2015-04-15 00:46:00.0">2015年4月15日</time>
</div>
<div class="meta-data--item">
<div class="meta-data--icon icon blog-icon comments"></div>
<a class="comments" href="/blog/2015/04/15/using-apache-kafka-for-integration-and-data-processing-pipelines-with-spring#disqus_thread" data-disqus-identifier="2080">
</a></div>
</div>
</header>
<div class="blog--post"><p>应用程序生成的数据比以往任何时候都要多，而挑战的很大一部分（甚至是在无法分析之前）首先要适应负载。<a href="https://kafka.apache.org">Apache的Kafka</a>迎接了这一挑战。它最初由LinkedIn设计，随后于2011年开源。该项目旨在提供一个统一的，高吞吐量，低延迟的平台来处理实时数据馈送。该设计在很大程度上受事务日志的影响。它是一个消息传递系统，类似于RabbitMQ，ActiveMQ，MQSeries等传统消息传递系统，但是非常适合日志聚合，持久性消息传递（每秒数百兆字节！）。读写，并且可以容纳许多客户。自然，这使其<em>非常</em>适合云规模的架构！</p>
<p>卡夫卡<a href="https://cwiki.apache.org/confluence/display/KAFKA/Powered+By">为许多大型生产系统提供动力</a> 。LinkedIn将其用于活动数据和运营指标，以为LinkedIn新闻提要，LinkedIn Today和进入Hadoop的离线分析提供支持。Twitter将其用作其流处理基础结构的一部分。Kafka支持在Foursquare进行在线到在线和在线到离线消息传递。它用于将Foursquare监视和生产系统与基于Hadoop的离线基础结构集成在一起。Square使用Kafka作为总线，通过Square的各种数据中心移动所有系统事件。这包括指标，日志，自定义事件等。在消费者方面，它可以输出到Splunk，Graphite或类似Esper的实时警报。Netflix每天使用它处理300-600BN条消息。Airbnb，Mozilla，Goldman Sachs，Tumblr，Yahoo，PayPal，Coursera，Urban Airship，Hotels.com以及其他大型网络明星中似乎无休止的列表也使用了它。显然，它可以保留在一些强大的系统中！</p><h2><a href="#installing-apache-kafka" class="anchor" name="installing-apache-kafka"></a>安装Apache Kafka</h2>
<p>有许多种安装Apache Kafka的方法。如果您使用的是OSX，并且使用的是Homebrew，则可以像<code>brew install kafka</code>一样简单。您也可以<a href="https://kafka.apache.org/downloads.html">从Apache下载最新发行版</a> 。我下载了<code>kafka_2.10-0.8.2.1.tgz</code>并<code>kafka_2.10-0.8.2.1.tgz</code>压缩，然后在其中找到了<a href="https://zookeeper.apache.org/">Apache Zookeeper</a>和Kafka的发行版，因此不需要其他内容。我在<code>$HOME</code>目录的另一个目录<code>bin</code>下安装了Apache Kafka，然后创建了一个环境变量<code>KAFKA_HOME</code> ，该变量指向<code>$HOME/bin/kafka</code> 。</p>
<p>首先启动Apache Zookeeper，指定所需的配置属性文件为：</p>
<pre><code class="prettyprint">$KAFKA_HOME/bin/zookeeper-server-start.sh  $KAFKA_HOME/config/zookeeper.properties

</code></pre>
<p>Apache Kafka发行版随附Zookeeper和Kafka的默认配置文件，这使上手变得容易。在更高级的用例中，您将需要自定义这些文件。</p>
<p>然后启动Apache Kafka。它也需要一个配置文件，如下所示：</p>
<pre><code class="prettyprint">$KAFKA_HOME/bin/kafka-server-start.sh  $KAFKA_HOME/config/server.properties
</code></pre>
<p><code>server.properties</code>文件包含以下内容的默认值：连接到Apache Zookeeper的位置的默认值（ <code>zookeeper.connect</code> ），应在套接字之间发送多少数据，默认情况下有多少个分区以及代理ID（ <code>broker.id</code>在整个集群中必须是唯一的）。</p>
<p>同一目录中还有其他脚本可用于发送和接收虚拟数据，这对于确定一切正常运行非常方便！</p>
<p>现在Apache Kafka已启动并正在运行，让我们看看从我们的应用程序中使用Apache Kafka。</p><h2><a href="#some-high-level-concepts" class="anchor" name="some-high-level-concepts"></a>一些高级概念</h2>
<p>Kafka <em>代理</em>群集由一台或多台服务器组成，其中每台服务器可能正在运行一个或多个代理进程。Apache Kafka旨在提供高可用性。没有<em>主</em>节点。所有节点都是可互换的。数据从一个节点复制到另一个节点，以确保在发生故障时仍然可用。</p>
<p>在Kafka中， <em>主题</em>是类别，类似于JMS目标或AMQP交换和队列。主题已分区，消息生产者可以选择将消息发送到哪个主题分区。分区中的每个消息都分配有唯一的顺序ID，即其<em>offset</em> 。更多的分区允许更大的并行使用，但这也将导致代理中的文件更多。</p>
<p><em>生产者</em>将消息发送到Apache Kafka代理主题，并指定要用于其产生的每条消息的分区。消息产生可以是同步的或异步的。生产者还指定他们想要哪种复制保证。</p>
<p><em>消费者</em>收听有关主题的消息并处理已发布消息的提要。如您所料，如果您使用了其他消息传递系统，通常这是（而且很有用！）异步。</p>
<p>像<a href="https://spring.io/projects/spring-xd">Spring XD</a>和许多其他分布式系统一样，Apache Kafka使用Apache Zookeeper来协调集群信息。Apache Zookeeper提供了一个共享的分层名称空间（称为<em>znodes</em> ），节点可以共享该名称空间以了解集群拓扑和可用性（这是<a href="https://github.com/spring-cloud/spring-cloud-zookeeper">Spring Cloud</a>即将对其提供支持的另一个原因。）。</p>
<p>Zookeeper在与Apache Kafka的交互中非常重要。例如，Apache Kafka有两个不同的API充当使用者。较高级别的API较容易上手，并且处理了处理分区等所有细微差别。它将需要引用Zookeeper实例以保持协调状态。</p>
<p>现在让我们转向在Spring中使用Apache Kafka。</p><h2><a href="#using-apache-kafka-with-spring-integration" class="anchor" name="using-apache-kafka-with-spring-integration"></a>结合使用Apache Kafka和Spring Integration</h2>
<p>最近发布的<a href="https://spring.io/blog/2015/03/26/spring-integration-kafka-support-1-1-ga-is-available">适用于Apache Kafka 1.1的Spring Integration</a>非常强大，并提供了入站适配器，可与较低级别的Apache Kafka API和较高级别的API一起使用。</p>
<p>当前，该适配器首先是XML配置，尽管针对该适配器的Spring Integration Java配置DSL工作已经在进行中，并且具有里程碑意义。现在，我们将在此处进行介绍。</p>
<p>为了使所有这些示例都能正常工作，我添加了<a href="https://repo.spring.io/simple/libs-milestone-local">libs-milestone-local Maven存储库</a>并使用以下依赖项：</p>
<ul>
<li>org.apache.kafka：kafka_2.10：0.8.1.1</li>
<li>org.springframework.boot：spring-boot-starter-integration：1.2.3。发布</li>
<li>org.springframework.boot：spring-boot-starter：1.2.3。发布</li>
<li>org.springframework.integration：spring-integration-kafka：1.1.1。发布</li>
<li>org.springframework.integration：spring-integration-java-dsl：1.1.0。M1</li>
</ul><h3><a href="#using-the-spring-integration-apache-kafka-with-the-spring-integration-xml-dsl" class="anchor" name="using-the-spring-integration-apache-kafka-with-the-spring-integration-xml-dsl"></a>将Spring Integration Apache Kafka与Spring Integration XML DSL结合使用</h3>
<p>首先，让我们看一下如何使用Spring Integration出站适配器发送<code>Message<T></code>实例从Spring Integration流到外部Apache Kafka实例。的例子是非常简单的：一个Spring集成<code>channel</code>命名<code>inputToKafka</code>充当管道转发<code>Message<T></code>消息发送到出站适配器<code>kafkaOutboundChannelAdapter</code> 。适配器本身可以从<code>kafka:producer-context</code>元素中指定的默认值中进行配置，也可以从适配器本地配置替代中获取它的配置。给定的<code>kafka:producer-context</code>元素中可能有一个或多个配置。</p>
<pre><code class="prettyprint xml"><?xml version="1.0" encoding="UTF-8"?>
<beans xmlns="http://www.springframework.org/schema/beans"
       xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
       xmlns:int="http://www.springframework.org/schema/integration"
       xmlns:int-kafka="http://www.springframework.org/schema/integration/kafka"
       xmlns:task="http://www.springframework.org/schema/task"
       xsi:schemaLocation="http://www.springframework.org/schema/integration/kafka http://www.springframework.org/schema/integration/kafka/spring-integration-kafka.xsd
		http://www.springframework.org/schema/integration http://www.springframework.org/schema/integration/spring-integration.xsd
		http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd
		http://www.springframework.org/schema/task http://www.springframework.org/schema/task/spring-task.xsd">

    <int:channel id="inputToKafka">
        <int:queue/>
    </int:channel>

    <int-kafka:outbound-channel-adapter
            id="kafkaOutboundChannelAdapter"
            kafka-producer-context-ref="kafkaProducerContext"
            channel="inputToKafka">
        <int:poller fixed-delay="1000" time-unit="MILLISECONDS" receive-timeout="0" task-executor="taskExecutor"/>
    </int-kafka:outbound-channel-adapter>

    <task:executor id="taskExecutor" pool-size="5" keep-alive="120" queue-capacity="500"/>

    <int-kafka:producer-context id="kafkaProducerContext">
        <int-kafka:producer-configurations>
            <int-kafka:producer-configuration broker-list="localhost:9092"
                                              topic="event-stream"
                                              compression-codec="default"/>
        </int-kafka:producer-configurations>
    </int-kafka:producer-context>

</beans>
</code></pre>
<p>这是来自Spring Boot应用程序的Java代码，用于通过将消息发送到传入的<code>inputToKafka</code> <code>MessageChannel</code>来触发使用出站适配器发送消息。</p>
<pre><code class="prettyprint java">package xml;

import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
import org.springframework.beans.factory.annotation.Qualifier;
import org.springframework.boot.CommandLineRunner;
import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.DependsOn;
import org.springframework.context.annotation.ImportResource;
import org.springframework.integration.config.EnableIntegration;
import org.springframework.messaging.MessageChannel;
import org.springframework.messaging.support.GenericMessage;

@SpringBootApplication
@EnableIntegration
@ImportResource("/xml/outbound-kafka-integration.xml")
public class DemoApplication {

    private Log log = LogFactory.getLog(getClass());

    @Bean
    @DependsOn("kafkaOutboundChannelAdapter")
    CommandLineRunner kickOff(@Qualifier("inputToKafka") MessageChannel in) {
        return args -> {
            for (int i = 0; i < 1000; i++) {
                in.send(new GenericMessage<>("#" + i));
                log.info("sending message #" + i);
            }
        };
    }

    public static void main(String args[]) {
        SpringApplication.run(DemoApplication.class, args);
    }
}

</code></pre><h3><a href="#using-the-new-apache-kafka-spring-integration-java-configuration-dsl" class="anchor" name="using-the-new-apache-kafka-spring-integration-java-configuration-dsl"></a>使用新的Apache Kafka Spring Integration Java配置DSL</h3>
<p>在Spring Integration 1.1发行之后不久，Spring Integration的摇滚明星<a href="https://spring.io/team/artembilan">Artem Bilan</a>致力于<a href="https://repo.spring.io/simple/libs-milestone-local/org/springframework/integration/spring-integration-java-dsl/1.1.0.M1/">添加Spring Integration Java Configuration DSL模拟</a> ，结果<a href="https://repo.spring.io/simple/libs-milestone-local/org/springframework/integration/spring-integration-java-dsl/1.1.0.M1/">真是</a>太美了！还不是GA（您现在需要添加<code>libs-milestone</code>存储库），但是我鼓励您尝试一下并尝试一下。它对我来说运作良好，并且Spring Integration团队始终渴望尽可能早地获得反馈！这是一个示例，演示了如何从两个不同的<code>IntegrationFlow</code>发送消息和使用消息。生产者类似于上面的示例XML。</p>
<p>在此示例中，新增功能是轮询用户。它以批处理为中心，并且将以固定的时间间隔拉出它看到的所有消息。在我们的代码中，收到的消息将是一个映射，该映射包含主题和作为其值的另一个映射，该映射具有分区ID和读取记录的批次（在这种情况下为10条记录）。有一个基于<code>MessageListenerContainer</code>的替代方法，可以在消息出现时对其进行处理。</p>
<pre><code class="prettyprint java">package jc;

import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.beans.factory.annotation.Qualifier;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.boot.CommandLineRunner;
import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.context.annotation.DependsOn;
import org.springframework.integration.IntegrationMessageHeaderAccessor;
import org.springframework.integration.config.EnableIntegration;
import org.springframework.integration.dsl.IntegrationFlow;
import org.springframework.integration.dsl.IntegrationFlows;
import org.springframework.integration.dsl.SourcePollingChannelAdapterSpec;
import org.springframework.integration.dsl.kafka.Kafka;
import org.springframework.integration.dsl.kafka.KafkaHighLevelConsumerMessageSourceSpec;
import org.springframework.integration.dsl.kafka.KafkaProducerMessageHandlerSpec;
import org.springframework.integration.dsl.support.Consumer;
import org.springframework.integration.kafka.support.ZookeeperConnect;
import org.springframework.messaging.MessageChannel;
import org.springframework.messaging.support.GenericMessage;
import org.springframework.stereotype.Component;

import java.util.List;
import java.util.Map;

/**
 * Demonstrates using the Spring Integration Apache Kafka Java Configuration DSL.
 * Thanks to Spring Integration ninja <a href="http://spring.io/team/artembilan">Artem Bilan</a>
 * for getting the Java Configuration DSL working so quickly!
 *
 * @author Josh Long
 */
@EnableIntegration
@SpringBootApplication
public class DemoApplication {

  public static final String TEST_TOPIC_ID = "event-stream";

  @Component
  public static class KafkaConfig {

    @Value("${kafka.topic:" + TEST_TOPIC_ID + "}")
    private String topic;

    @Value("${kafka.address:localhost:9092}")
    private String brokerAddress;

    @Value("${zookeeper.address:localhost:2181}")
    private String zookeeperAddress;

    KafkaConfig() {
    }

    public KafkaConfig(String t, String b, String zk) {
        this.topic = t;
        this.brokerAddress = b;
        this.zookeeperAddress = zk;
    }

    public String getTopic() {
        return topic;
    }

    public String getBrokerAddress() {
        return brokerAddress;
    }

    public String getZookeeperAddress() {
        return zookeeperAddress;
    }
  }

  @Configuration
  public static class ProducerConfiguration {

    @Autowired
    private KafkaConfig kafkaConfig;

    private static final String OUTBOUND_ID = "outbound";

    private Log log = LogFactory.getLog(getClass());

    @Bean
    @DependsOn(OUTBOUND_ID)
    CommandLineRunner kickOff( 
           @Qualifier(OUTBOUND_ID + ".input") MessageChannel in) {
        return args -> {
            for (int i = 0; i < 1000; i++) {
                in.send(new GenericMessage<>("#" + i));
                log.info("sending message #" + i);
            }
        };
    }

    @Bean(name = OUTBOUND_ID)
    IntegrationFlow producer() {

      log.info("starting producer flow..");
      return flowDefinition -> {

        Consumer<KafkaProducerMessageHandlerSpec.ProducerMetadataSpec> spec =
          (KafkaProducerMessageHandlerSpec.ProducerMetadataSpec metadata)->
            metadata.async(true)
              .batchNumMessages(10)
              .valueClassType(String.class)
              .<String>valueEncoder(String::getBytes);

        KafkaProducerMessageHandlerSpec messageHandlerSpec =
          Kafka.outboundChannelAdapter(
               props -> props.put("queue.buffering.max.ms", "15000"))
            .messageKey(m -> m.getHeaders().get(IntegrationMessageHeaderAccessor.SEQUENCE_NUMBER))
            .addProducer(this.kafkaConfig.getTopic(), 
                this.kafkaConfig.getBrokerAddress(), spec);
        flowDefinition
            .handle(messageHandlerSpec);
      };
    }
  }

  @Configuration
  public static class ConsumerConfiguration {

    @Autowired
    private KafkaConfig kafkaConfig;

    private Log log = LogFactory.getLog(getClass());

    @Bean
    IntegrationFlow consumer() {

      log.info("starting consumer..");

      KafkaHighLevelConsumerMessageSourceSpec messageSourceSpec = Kafka.inboundChannelAdapter(
          new ZookeeperConnect(this.kafkaConfig.getZookeeperAddress()))
            .consumerProperties(props ->
                props.put("auto.offset.reset", "smallest")
                     .put("auto.commit.interval.ms", "100"))
            .addConsumer("myGroup", metadata -> metadata.consumerTimeout(100)
              .topicStreamMap(m -> m.put(this.kafkaConfig.getTopic(), 1))
              .maxMessages(10)
              .valueDecoder(String::new));

      Consumer<SourcePollingChannelAdapterSpec> endpointConfigurer = e -> e.poller(p -> p.fixedDelay(100));

      return IntegrationFlows
        .from(messageSourceSpec, endpointConfigurer)
        .<Map<String, List<String>>>handle((payload, headers) -> {
            payload.entrySet().forEach(e -> log.info(e.getKey() + '=' + e.getValue()));
            return null;
        })
        .get();
    }
  }

  public static void main(String[] args) {
      SpringApplication.run(DemoApplication.class, args);
  }
}

</code></pre>
<p>该示例大量使用Java 8 lambda。</p>
<p>生产者花费一些时间来确定在一次发送操作中将发送多少消息，如何对键和值进行编码（毕竟，Kafka只知道<code>byte[]</code>数组）以及应该同步还是异步发送消息。在下一行中，我们将配置出站适配器本身，然后定义一个<code>IntegrationFlow</code> ，以使所有消息都通过Kafka出站适配器发送出去。</p>
<p>消费者花费一些时间来确定要连接到哪个Zookeeper实例，每批接收多少消息（10），等等。一旦收到消息批，它们将被传递到我传入的<code>handle</code>方法中一个lambda，它将枚举有效载荷的主体并将其打印出来。没有什么花哨。</p><h2><a href="#using-apache-kafka-with-spring-xd" class="anchor" name="using-apache-kafka-with-spring-xd"></a>在Spring XD中使用Apache Kafka</h2>
<p>Apache Kafka是一条消息总线，用作集成总线时可能非常强大。但是，它确实具有自己的优势，因为它足够快且可扩展，可用于通过处理管道路由大数据。而且，如果您正在执行数据处理，那么您真的想要<a href="https://projects.spring.io/spring-xd/">Spring XD</a> ！Spring XD使使用Apache Kafka变得非常简单（因为支持是基于Apache Kafka Spring Integration适配器构建的！）在复杂的流处理管道中。Apache Kafka公开为Spring XD <em>源</em> -数据来自-接收器-数据到达。</p>
<img src="https://projects.spring.io/spring-xd/img/spring-xd-unified-platform-for-big-data.png">
<p>Spring XD公开了一种超级方便的DSL，用于创建类似<code>bash</code>的管道和过滤器流。Spring XD是一个集中式运行时，用于管理，扩展和监视数据处理作业。它建立在Spring Integration，Spring Batch，Spring Data和Spring for Hadoop之上，成为一站式数据处理商店。Spring XD Jobs从<em>源</em>读取数据，通过可能对数据进行计数，过滤，丰富或转换的处理组件运行它们，然后将它们写入接收器。</p>
<p>Spring Integration和Spring XD的忍者<a href="https://twitter.com/mariusbogoevici">Marius Bogoevici</a>在Apache Kafka的Spring Integration和Spring XD实现中做了很多最新工作，他们汇集了一个非常不错的示例，展示了<a href="https://github.com/spring-projects/spring-xd-samples/tree/master/kafka-source">如何使Spring XD和Kafka流程完全正常工作</a> 。<code>README</code>引导您<code>README</code>完成Apache Kafka，Spring XD和所有必需主题的设置。但是，本质是当您使用Spring XD Shell和Shell DSL组成流时。Spring XD组件是已预先配置的命名组件，但是具有许多参数，您可以通过XD Shell和DSL用<code>--..</code>参数覆盖这些参数。 （顺便说一下，那个DSL是由令人惊叹的Spring Expression语言成名的<a href="https://spring.io/team/aclement">Andy Clement</a>编写的！）这是一个示例，该示例将流配置为从Apache Kafka源读取数据，然后将消息写入名为<code>log</code>的组件，该组件是一个接收器。在这种情况下， <code>log</code>可以是syslogd，Splunk，HDFS等。</p>
<pre><code class="prettyprint bash">xd> stream create kafka-source-test --definition "kafka --zkconnect=localhost:2181 --topic=event-stream | log" --deploy

</code></pre>
<p>就是这样！自然，这只是Spring XD的小巧之处，但希望您会同意诱人的可能性。</p><h2><a href="#deploying-a-kafka-server-with-lattice-and-docker" class="anchor" name="deploying-a-kafka-server-with-lattice-and-docker"></a>使用莱迪思和Docker部署Kafka服务器</h2>
<p>使用<a href="http://lattice.cf">Lattice</a>可以很容易地获得Kafka安装示例的所有安装程序， <a href="http://lattice.cf">Lattice</a>是一种分布式运行时，该运行时除其他容器格式外还支持非常流行的Docker映像格式。<a href="https://github.com/spotify/docker-kafka">Spotify提供了一个Docker映像，该映像设置了并置的Zookeeper和Kafka映像</a> 。您可以轻松地将其部署到莱迪思集群，如下所示：</p>
<pre><code class="prettyprint bash">ltc create --run-as-root m-kafka spotify/kafka
</code></pre>
<p>从那里，您可以轻松扩展Apache Kafka实例，甚至还可以更轻松地从基于云的服务中使用Apache Kafka。</p><h2><a href="#next-steps" class="anchor" name="next-steps"></a>下一步</h2>
<p>您可以<a href="https://github.com/joshlong/spring-and-kafka">在我的GitHub帐户上</a>找到<a href="https://github.com/joshlong/spring-and-kafka">此博客</a>的代码。</p>
<p>我们只刮了一下表面！</p>
<p>如果您想了解更多（为什么不呢？），那么一定要检查一下Marius Bogoevici和Mark Pollack博士即将举行的<a href="https://spring.io/blog/2015/03/17/webinar-reactive-data-pipelines-with-spring-xd-and-kafka">关于使用Spring XD和Apache Kafka的Reactive数据管道的网络研讨会</a> ，他们将演示它的简便性。可以使用RxJava，Spring XD和Apache Kafka！</p></div>
</div>
<section id="disqus_thread"></section>
<script type="text/javascript">var disqus_shortname = 'spring-io';
      var disqus_identifier = 2080;

      (function(disqus_shortname, document) {
        injectScript('//' + disqus_shortname + '.disqus.com/embed.js');
        injectScript('//' + disqus_shortname + '.disqus.com/count.js');

        function injectScript(url) {
          var s = document.createElement('script');
          s.async = true;
          s.src = url;
          (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(s);
        }

      }(disqus_shortname, document));</script>
<noscript>请启用JavaScript以查看<a href="http://disqus.com/?ref_noscript">由Disqus提供</a>的<a href="http://disqus.com/?ref_noscript">评论。</a></noscript>
<a class="dsq-brlink" href="https://disqus.com">由<span class="logo-disqus">Disqus</span>提供动力的评论</a>
<div class="mobile-only">
<p><a href="/blog">
<i class="icon-chevron-left"></i>背部</a></p>
</div>
</article>
<aside class="span4 mobile-right-pane" id="sidebar">
<div>
<ul class="right-pane-widget--container secondary-nav with-icon">
<li class="blog-category">
<div class="icon blog-icon all-posts"></div>
<a href="/blog">所有帖子</a> <a class="pull-right" href="/blog.atom"><i class="icon-rss"></i></a>
</li>
<li class="blog-category active">
<div class="icon blog-icon engineering"></div>
<a href="/blog/category/engineering">工程</a> <a class="pull-right" href="/blog/category/engineering.atom"><i class="icon-rss"></i></a>
</li>
<li class="blog-category">
<div class="icon blog-icon releases"></div>
<a href="/blog/category/releases">发布</a> <a class="pull-right" href="/blog/category/releases.atom"><i class="icon-rss"></i></a>
</li>
<li class="blog-category">
<div class="icon blog-icon news-and-events"></div>
<a href="/blog/category/news">新闻与活动</a> <a class="pull-right" href="/blog/category/news.atom"><i class="icon-rss"></i></a>
</li>
</ul>
<ul class="social-btn--container">
<a class="social-btn twitter" href="https://twitter.com/springcentral"></a>

<a class="social-btn linkedin" href="https://www.linkedin.com/groups/46964"></a>
<a class="social-btn youtube" href="/videos"></a>
</ul>
<div id="blog-sidebar-newsletter">
<p>将The Spring Team的更新发送到您的收件箱</p>
<script src="https://app-sj05.marketo.com/js/forms2/js/forms2.min.js"></script>
<form id="mktoForm_4723"></form>
<script>MktoForms2.loadForm("https://app-sj05.marketo.com", "625-IUJ-009", 4723, function(form){
          form.onSuccess(function(values, followUpUrl) {
            form.getFormElem().html("<p>Thank you!</p>");
            return false;
          });
        });</script>
</div>
</div>
</aside>
</div>
</div>
</div>
<footer class="footer">
<div class="container-fluid">
<div class="row-fluid">
<div class="span12">
<div class="navbar">
<div class="container">
<ul class="nav">
<li><a href="/team">球队</a></li>
<li><a href="/tools">工具类</a></li>
<li><a href="https://store.pivotal.io/">商店</a></li>
<li><a href="/blog">通讯</a></li>
</ul>
</div>
</div>© <span>2019</span> <a href="https://www.pivotal.io/">Pivotal Software</a> ，Inc.保留所有权利。
<a href="https://pivotal.io/terms-of-use">使用条款</a> • <a href="https://pivotal.io/privacy-policy">隐私</a> • <a href="/trademarks">商标准则</a>
<div id="teconsent" style="display:inline-block"></div>
</div>
</div>
</div>
</footer>
<div id="scrim"></div>
</div>
</body></html>